import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
import nltk
from nltk.stem.snowball import SnowballStemmer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

# Loading the data set - training data.
twenty_train = fetch_20newsgroups(subset='train', shuffle=True)

# ä»æ–‡æœ¬æ–‡ä»¶ä¸­æå–ç‰¹å¾
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(twenty_train.data)
# print(X_train_counts)
# TF-IDF
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
print(X_train_tfidf)
# æœºå™¨å­¦ä¹ 
# åœ¨è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒæœ´ç´ è´å¶æ–¯ï¼ˆNBï¼‰åˆ†ç±»å™¨ã€‚
clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)

# å»ºç«‹ç®¡é“ï¼šé€šè¿‡å¦‚ä¸‹æ„å»ºç®¡é“ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™æ›´å°‘çš„ä»£ç å¹¶å®Œæˆä¸Šè¿°æ‰€æœ‰æ“ä½œï¼š
# åç§°â€œ vectâ€ï¼Œâ€œ tfidfâ€å’Œâ€œ clfâ€æ˜¯ä»»æ„çš„ï¼Œä½†å°†åœ¨ä»¥åä½¿ç”¨ã€‚
# æˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨'text_clf'ã€‚
text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', MultinomialNB())])
text_clf = text_clf.fit(twenty_train.data, twenty_train.target)

# NBåˆ†ç±»å™¨çš„æ€§èƒ½
twenty_test = fetch_20newsgroups(subset='test', shuffle=True)
predicted = text_clf.predict(twenty_test.data)
np.mean(predicted == twenty_test.target)

# è®­ç»ƒæ”¯æŒå‘é‡æœº-SVMå¹¶è®¡ç®—å…¶æ€§èƒ½
text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),
                         ('clf-svm',
                          SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5, random_state=42))])


text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)
predicted_svm = text_clf_svm.predict(twenty_test.data)
np.mean(predicted_svm == twenty_test.target)

# ç½‘æ ¼æœç´¢
# åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ­£åœ¨åˆ›å»ºè¦è¿›è¡Œæ€§èƒ½è°ƒæ•´çš„å‚æ•°åˆ—è¡¨ã€‚
# æ‰€æœ‰å‚æ•°åç§°å‡ä»¥åˆ†ç±»å™¨åç§°å¼€å¤´ï¼ˆè®°ä½æˆ‘ä»¬ç»™å®šçš„ä»»æ„åç§°ï¼‰ã€‚
# ä¾‹å¦‚ vect__ngram_range; åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¦å‘Šè¯‰æ‚¨ä½¿ç”¨unigramå’Œbigramså¹¶é€‰æ‹©æœ€ä½³çš„ã€‚
parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}

# æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡ä¼ é€’åˆ†ç±»å™¨ï¼Œå‚æ•°
# å’Œn_jobs = -1æ¥åˆ›å»ºç½‘æ ¼æœç´¢çš„å®ä¾‹ï¼Œè¯¥åˆ†ç±»å™¨å‘Šè¯‰æ‚¨ä½¿ç”¨ç”¨æˆ·è®¡ç®—æœºä¸­çš„å¤šä¸ªå†…æ ¸ã€‚

gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)

# è¦æŸ¥çœ‹æœ€ä½³å¹³å‡å¾—åˆ†å’Œå‚æ•°ï¼Œè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼ƒä¸Šé¢çš„è¾“å‡ºåº”ä¸ºï¼šNBåˆ†ç±»å™¨çš„ç²¾åº¦ç°åœ¨å·²æé«˜åˆ°ã€œ90.6ï¼…ï¼ˆä¸å†æ˜¯å¤©çœŸäº†ï¼ğŸ˜„ï¼‰
# å’Œç›¸åº”çš„å‚æ•°ä¸º{ 'clf__alpha'ï¼š0.01ï¼Œ'tfidf__use_idf'ï¼šTrueï¼Œ'vect__ngram_range'ï¼šï¼ˆ1ã€2ï¼‰}ã€‚

parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),
                  'clf-svm__alpha': (1e-2, 1e-3)}

gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)
gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)

# NLTK
# Removing stop words

text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()),
                     ('clf', MultinomialNB())])

# Stemming Code


stemmer = SnowballStemmer("english", ignore_stopwords=True)


class StemmedCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self).build_analyzer()
        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])


stemmed_count_vect = StemmedCountVectorizer(stop_words='english')

text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()),
                             ('mnb', MultinomialNB(fit_prior=False))])

text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)

predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)

print(np.mean(predicted_mnb_stemmed == twenty_test.target))
